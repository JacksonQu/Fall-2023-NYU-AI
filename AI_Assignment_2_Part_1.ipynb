{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWhpeXxaAG1b",
        "outputId": "2acfc595-f158-4b07-cc91-bc0eb0fe643f"
      },
      "outputs": [],
      "source": [
        "# Get IMDB dataset\n",
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "35wORGsZANAY"
      },
      "outputs": [],
      "source": [
        "#1 Create the BoW feature vectors\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import os\n",
        "\n",
        "# Define the path to the dataset\n",
        "dataset_path = 'aclImdb/'\n",
        "\n",
        "# Load the dataset\n",
        "def load_dataset(split):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    for category in ['pos', 'neg']:\n",
        "        category_path = os.path.join(dataset_path, split, category)\n",
        "        for filename in os.listdir(category_path):\n",
        "            with open(os.path.join(category_path, filename), 'r', encoding='utf-8') as file:\n",
        "                text = file.read()\n",
        "                texts.append(text)\n",
        "                labels.append(1 if category == 'pos' else 0)\n",
        "    return texts, labels\n",
        "\n",
        "# Load the training and testing datasets\n",
        "train_texts, train_labels = load_dataset('train')\n",
        "test_texts, test_labels = load_dataset('test')\n",
        "\n",
        "# Initialize the CountVectorizer\n",
        "vectorizer = CountVectorizer(max_features=5000)  # You can adjust max_features if needed\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train = vectorizer.fit_transform(train_texts)\n",
        "\n",
        "# Transform the testing data\n",
        "X_test = vectorizer.transform(test_texts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HwZ0RTJAnl9",
        "outputId": "53c7505a-9a2e-4bca-e7f9-176a872fda8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression - Training Accuracy: 0.9639\n",
            "Logistic Regression - Testing Accuracy: 0.8514\n"
          ]
        }
      ],
      "source": [
        "#2. Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model\n",
        "lr_model.fit(X_train, train_labels)\n",
        "\n",
        "# Evaluate the model\n",
        "lr_train_accuracy = lr_model.score(X_train, train_labels)\n",
        "lr_test_accuracy = lr_model.score(X_test, test_labels)\n",
        "\n",
        "print(f\"Logistic Regression - Training Accuracy: {lr_train_accuracy:.4f}\")\n",
        "print(f\"Logistic Regression - Testing Accuracy: {lr_test_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBx3vw6cIRZN",
        "outputId": "42463e0c-5476-45f5-8553-b84ec0a42fa4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-10-04 13:18:14.715909: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-10-04 13:18:14.770085: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-10-04 13:18:14.770126: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-10-04 13:18:14.770152: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-10-04 13:18:14.779175: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-10-04 13:18:18.662542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12637 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0\n",
            "2023-10-04 13:18:18.663238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 29991 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:89:00.0, compute capability: 7.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-10-04 13:18:22.480681: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f6de3b1d800 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2023-10-04 13:18:22.480720: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
            "2023-10-04 13:18:22.480726: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
            "2023-10-04 13:18:22.486440: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2023-10-04 13:18:22.655935: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n",
            "2023-10-04 13:18:22.789076: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 8s 7ms/step - loss: 0.3480 - accuracy: 0.8571 - val_loss: 0.3207 - val_accuracy: 0.8724\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.2320 - accuracy: 0.9091 - val_loss: 0.3178 - val_accuracy: 0.8741\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.1773 - accuracy: 0.9323 - val_loss: 0.3167 - val_accuracy: 0.8706\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.1206 - accuracy: 0.9574 - val_loss: 0.3662 - val_accuracy: 0.8671\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.0673 - accuracy: 0.9796 - val_loss: 0.4416 - val_accuracy: 0.8588\n",
            "MLP - Training Accuracy: 0.9918\n",
            "MLP - Testing Accuracy: 0.8588\n"
          ]
        }
      ],
      "source": [
        "# MLP\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "# Convert sparse matrix to dense numpy array\n",
        "X_train_dense = X_train.toarray()\n",
        "X_test_dense = X_test.toarray()\n",
        "\n",
        "# Initialize the MLP model\n",
        "mlp_model = Sequential([\n",
        "    Dense(64, activation='relu', input_dim=X_train.shape[1]),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "mlp_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Convert labels to numpy array\n",
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "# Train the model\n",
        "mlp_model.fit(X_train_dense, train_labels, epochs=5, batch_size=32, validation_data=(X_test_dense, test_labels))\n",
        "\n",
        "# Evaluate the model\n",
        "mlp_train_accuracy = mlp_model.evaluate(X_train_dense, train_labels, verbose=0)[1]\n",
        "mlp_test_accuracy = mlp_model.evaluate(X_test_dense, test_labels, verbose=0)[1]\n",
        "\n",
        "print(f\"MLP - Training Accuracy: {mlp_train_accuracy:.4f}\")\n",
        "print(f\"MLP - Testing Accuracy: {mlp_test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhpMoVBXGHgp",
        "outputId": "9e82c44a-814e-4c6d-f5d3-e523ce1e5f7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 38s 46ms/step - loss: 0.4595 - accuracy: 0.7764 - val_loss: 0.3518 - val_accuracy: 0.8428\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 0.2754 - accuracy: 0.8871 - val_loss: 0.3211 - val_accuracy: 0.8595\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 9s 12ms/step - loss: 0.1878 - accuracy: 0.9302 - val_loss: 0.3418 - val_accuracy: 0.8581\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 9s 11ms/step - loss: 0.1140 - accuracy: 0.9626 - val_loss: 0.3919 - val_accuracy: 0.8544\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 8s 11ms/step - loss: 0.0589 - accuracy: 0.9845 - val_loss: 0.4646 - val_accuracy: 0.8500\n",
            "CNN - Training Accuracy: 0.9968\n",
            "CNN - Testing Accuracy: 0.8500\n"
          ]
        }
      ],
      "source": [
        "# CNN\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Tokenize and pad the sequences\n",
        "max_words = 5000\n",
        "maxlen = 100  # Adjust as needed\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "X_train = tokenizer.texts_to_sequences(train_texts)\n",
        "X_test = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
        "\n",
        "# Initialize the CNN model with embedding layer\n",
        "cnn_model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=50, input_length=maxlen),\n",
        "    Conv1D(128, 5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Convert labels to numpy array\n",
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "# Train the model\n",
        "cnn_model.fit(X_train, train_labels, epochs=5, batch_size=32, validation_data=(X_test, test_labels))\n",
        "\n",
        "# Evaluate the model\n",
        "cnn_train_accuracy = cnn_model.evaluate(X_train, train_labels, verbose=0)[1]\n",
        "cnn_test_accuracy = cnn_model.evaluate(X_test, test_labels, verbose=0)[1]\n",
        "\n",
        "print(f\"CNN - Training Accuracy: {cnn_train_accuracy:.4f}\")\n",
        "print(f\"CNN - Testing Accuracy: {cnn_test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E77fKjxNHdW-",
        "outputId": "d632b1d5-ef3b-481c-8406-45162f7dc07a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-04 18:53:25,514] A new study created in memory with name: no-name-3b54b0ab-6121-435d-b22c-87356bb65c4b\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-10-04 18:53:25,606] Trial 0 finished with value: 0.5116 and parameters: {'C': 97285.13274100378}. Best is trial 0 with value: 0.5116.\n",
            "[I 2023-10-04 18:53:25,703] Trial 1 finished with value: 0.5116 and parameters: {'C': 44666.142708819956}. Best is trial 0 with value: 0.5116.\n",
            "[I 2023-10-04 18:53:25,835] Trial 2 finished with value: 0.5116 and parameters: {'C': 35911.804711238874}. Best is trial 0 with value: 0.5116.\n",
            "[I 2023-10-04 18:53:25,975] Trial 3 finished with value: 0.5116 and parameters: {'C': 17140.220305938765}. Best is trial 0 with value: 0.5116.\n",
            "[I 2023-10-04 18:53:26,119] Trial 4 finished with value: 0.5116 and parameters: {'C': 48814.04590725136}. Best is trial 0 with value: 0.5116.\n",
            "[I 2023-10-04 18:53:26,121] A new study created in memory with name: no-name-92d6f70b-c0e9-40f0-9d0d-7d7bed6b97e9\n",
            "[I 2023-10-04 18:54:55,519] Trial 0 finished with value: 0.8367999792098999 and parameters: {'learning_rate': 0.0004145185953903112}. Best is trial 0 with value: 0.8367999792098999.\n",
            "[I 2023-10-04 18:56:24,320] Trial 1 finished with value: 0.8571199774742126 and parameters: {'learning_rate': 0.0031342443940478447}. Best is trial 1 with value: 0.8571199774742126.\n",
            "[I 2023-10-04 18:57:54,686] Trial 2 finished with value: 0.8354399800300598 and parameters: {'learning_rate': 0.00927749883984349}. Best is trial 1 with value: 0.8571199774742126.\n",
            "[I 2023-10-04 18:59:24,955] Trial 3 finished with value: 0.8050000071525574 and parameters: {'learning_rate': 0.004464153377254874}. Best is trial 1 with value: 0.8571199774742126.\n",
            "[I 2023-10-04 19:00:54,171] Trial 4 finished with value: 0.8553599715232849 and parameters: {'learning_rate': 0.0021635264261714543}. Best is trial 1 with value: 0.8571199774742126.\n",
            "[I 2023-10-04 19:02:22,938] Trial 5 finished with value: 0.8383200168609619 and parameters: {'learning_rate': 0.00045300914148055463}. Best is trial 1 with value: 0.8571199774742126.\n",
            "[I 2023-10-04 19:03:53,670] Trial 6 finished with value: 0.8420000076293945 and parameters: {'learning_rate': 0.004328277526888495}. Best is trial 1 with value: 0.8571199774742126.\n",
            "[I 2023-10-04 19:05:23,189] Trial 7 finished with value: 0.8441200256347656 and parameters: {'learning_rate': 0.004205111793257112}. Best is trial 1 with value: 0.8571199774742126.\n",
            "[I 2023-10-04 19:06:52,108] Trial 8 finished with value: 0.8293200135231018 and parameters: {'learning_rate': 0.009300170897749522}. Best is trial 1 with value: 0.8571199774742126.\n",
            "[I 2023-10-04 19:08:21,872] Trial 9 finished with value: 0.8569200038909912 and parameters: {'learning_rate': 0.0030371216670016334}. Best is trial 1 with value: 0.8571199774742126.\n",
            "[I 2023-10-04 19:09:51,408] Trial 10 finished with value: 0.83788001537323 and parameters: {'learning_rate': 0.006913017167079623}. Best is trial 1 with value: 0.8571199774742126.\n",
            "[I 2023-10-04 19:11:20,615] Trial 11 finished with value: 0.8566399812698364 and parameters: {'learning_rate': 0.002466651500105128}. Best is trial 1 with value: 0.8571199774742126.\n",
            "[I 2023-10-04 19:12:50,216] Trial 12 finished with value: 0.8565199971199036 and parameters: {'learning_rate': 0.0028199427700951137}. Best is trial 1 with value: 0.8571199774742126.\n",
            "[I 2023-10-04 19:14:18,905] Trial 13 finished with value: 0.8452799916267395 and parameters: {'learning_rate': 0.00641491727590269}. Best is trial 1 with value: 0.8571199774742126.\n",
            "[I 2023-10-04 19:15:47,609] Trial 14 finished with value: 0.8568400144577026 and parameters: {'learning_rate': 0.003117204790401037}. Best is trial 1 with value: 0.8571199774742126.\n",
            "[I 2023-10-04 19:17:17,270] Trial 15 finished with value: 0.8561599850654602 and parameters: {'learning_rate': 0.0016085672545007668}. Best is trial 1 with value: 0.8571199774742126.\n",
            "[I 2023-10-04 19:18:47,121] Trial 16 finished with value: 0.8569599986076355 and parameters: {'learning_rate': 0.003525388586907946}. Best is trial 1 with value: 0.8571199774742126.\n",
            "[I 2023-10-04 19:20:16,428] Trial 17 finished with value: 0.8468800187110901 and parameters: {'learning_rate': 0.005576721072910993}. Best is trial 1 with value: 0.8571199774742126.\n",
            "[I 2023-10-04 19:21:46,955] Trial 18 finished with value: 0.854640007019043 and parameters: {'learning_rate': 0.001712712464988676}. Best is trial 1 with value: 0.8571199774742126.\n",
            "[I 2023-10-04 19:23:16,370] Trial 19 finished with value: 0.8456400036811829 and parameters: {'learning_rate': 0.0035265475490882502}. Best is trial 1 with value: 0.8571199774742126.\n",
            "[I 2023-10-04 19:24:45,987] Trial 20 finished with value: 0.8428000211715698 and parameters: {'learning_rate': 0.005433590142877092}. Best is trial 1 with value: 0.8571199774742126.\n",
            "[I 2023-10-04 19:26:15,468] Trial 21 finished with value: 0.8610000014305115 and parameters: {'learning_rate': 0.0031619903466632814}. Best is trial 21 with value: 0.8610000014305115.\n",
            "[I 2023-10-04 19:27:44,479] Trial 22 finished with value: 0.8411999940872192 and parameters: {'learning_rate': 0.0036879138469682452}. Best is trial 21 with value: 0.8610000014305115.\n",
            "[I 2023-10-04 19:29:14,209] Trial 23 finished with value: 0.8489999771118164 and parameters: {'learning_rate': 0.0012807621205713088}. Best is trial 21 with value: 0.8610000014305115.\n",
            "[I 2023-10-04 19:30:42,742] Trial 24 finished with value: 0.8577600121498108 and parameters: {'learning_rate': 0.0023637196952380557}. Best is trial 21 with value: 0.8610000014305115.\n",
            "[I 2023-10-04 19:32:11,923] Trial 25 finished with value: 0.8588399887084961 and parameters: {'learning_rate': 0.0021694532256265504}. Best is trial 21 with value: 0.8610000014305115.\n",
            "[I 2023-10-04 19:33:42,099] Trial 26 finished with value: 0.8468400239944458 and parameters: {'learning_rate': 0.0010289958489996674}. Best is trial 21 with value: 0.8610000014305115.\n",
            "[I 2023-10-04 19:35:12,338] Trial 27 finished with value: 0.8559600114822388 and parameters: {'learning_rate': 0.002238317340831443}. Best is trial 21 with value: 0.8610000014305115.\n",
            "[I 2023-10-04 19:36:42,476] Trial 28 finished with value: 0.8599600195884705 and parameters: {'learning_rate': 0.002368728487280651}. Best is trial 21 with value: 0.8610000014305115.\n",
            "[I 2023-10-04 19:38:13,553] Trial 29 finished with value: 0.7683600187301636 and parameters: {'learning_rate': 2.1156853329342556e-05}. Best is trial 21 with value: 0.8610000014305115.\n",
            "[I 2023-10-04 19:39:42,688] Trial 30 finished with value: 0.836359977722168 and parameters: {'learning_rate': 0.0005452958916952128}. Best is trial 21 with value: 0.8610000014305115.\n",
            "[I 2023-10-04 19:41:12,442] Trial 31 finished with value: 0.8566399812698364 and parameters: {'learning_rate': 0.00205799267672121}. Best is trial 21 with value: 0.8610000014305115.\n",
            "[I 2023-10-04 19:42:43,141] Trial 32 finished with value: 0.8579599857330322 and parameters: {'learning_rate': 0.0025364053573085576}. Best is trial 21 with value: 0.8610000014305115.\n",
            "[I 2023-10-04 19:44:12,928] Trial 33 finished with value: 0.8476799726486206 and parameters: {'learning_rate': 0.00111239569367413}. Best is trial 21 with value: 0.8610000014305115.\n",
            "[I 2023-10-04 19:45:44,123] Trial 34 finished with value: 0.8576800227165222 and parameters: {'learning_rate': 0.0028324731573532123}. Best is trial 21 with value: 0.8610000014305115.\n",
            "[I 2023-10-04 19:47:14,434] Trial 35 finished with value: 0.855239987373352 and parameters: {'learning_rate': 0.001828332407402083}. Best is trial 21 with value: 0.8610000014305115.\n",
            "[I 2023-10-04 19:48:44,909] Trial 36 finished with value: 0.8598399758338928 and parameters: {'learning_rate': 0.002556486060190159}. Best is trial 21 with value: 0.8610000014305115.\n",
            "[I 2023-10-04 19:50:14,257] Trial 37 finished with value: 0.8492799997329712 and parameters: {'learning_rate': 0.0038556423554773635}. Best is trial 21 with value: 0.8610000014305115.\n",
            "[I 2023-10-04 19:51:44,876] Trial 38 finished with value: 0.8544800281524658 and parameters: {'learning_rate': 0.0017419910793841416}. Best is trial 21 with value: 0.8610000014305115.\n",
            "[I 2023-10-04 19:53:14,311] Trial 39 finished with value: 0.8590400218963623 and parameters: {'learning_rate': 0.003179072994423318}. Best is trial 21 with value: 0.8610000014305115.\n",
            "[I 2023-10-04 19:54:44,681] Trial 40 finished with value: 0.8467599749565125 and parameters: {'learning_rate': 0.004478475265796918}. Best is trial 21 with value: 0.8610000014305115.\n",
            "[I 2023-10-04 19:56:15,176] Trial 41 finished with value: 0.8587200045585632 and parameters: {'learning_rate': 0.0032005093368780305}. Best is trial 21 with value: 0.8610000014305115.\n",
            "[I 2023-10-04 19:57:45,969] Trial 42 finished with value: 0.8561999797821045 and parameters: {'learning_rate': 0.002702788198648946}. Best is trial 21 with value: 0.8610000014305115.\n",
            "[I 2023-10-04 19:59:16,061] Trial 43 finished with value: 0.8577200174331665 and parameters: {'learning_rate': 0.0021654658666188107}. Best is trial 21 with value: 0.8610000014305115.\n",
            "[I 2023-10-04 20:00:45,864] Trial 44 finished with value: 0.8442400097846985 and parameters: {'learning_rate': 0.0032266655565797303}. Best is trial 21 with value: 0.8610000014305115.\n",
            "[I 2023-10-04 20:02:16,058] Trial 45 finished with value: 0.8440399765968323 and parameters: {'learning_rate': 0.003956135733077518}. Best is trial 21 with value: 0.8610000014305115.\n",
            "[I 2023-10-04 20:03:45,695] Trial 46 finished with value: 0.8580800294876099 and parameters: {'learning_rate': 0.0025681191779801256}. Best is trial 21 with value: 0.8610000014305115.\n",
            "[I 2023-10-04 20:05:15,791] Trial 47 finished with value: 0.8580399751663208 and parameters: {'learning_rate': 0.0030055959032522415}. Best is trial 21 with value: 0.8610000014305115.\n",
            "[I 2023-10-04 20:06:44,925] Trial 48 finished with value: 0.8442400097846985 and parameters: {'learning_rate': 0.004214226108793285}. Best is trial 21 with value: 0.8610000014305115.\n",
            "[I 2023-10-04 20:08:14,732] Trial 49 finished with value: 0.8506399989128113 and parameters: {'learning_rate': 0.00142387375751608}. Best is trial 21 with value: 0.8610000014305115.\n",
            "[I 2023-10-04 20:08:14,737] A new study created in memory with name: no-name-6988bf4b-c76f-4c12-83e3-596a2cace349\n",
            "[I 2023-10-04 20:09:45,671] Trial 0 finished with value: 0.563040018081665 and parameters: {'learning_rate': 0.004611629894650763}. Best is trial 0 with value: 0.563040018081665.\n",
            "[I 2023-10-04 20:11:02,562] Trial 1 finished with value: 0.6027200222015381 and parameters: {'learning_rate': 0.006513125203832794}. Best is trial 1 with value: 0.6027200222015381.\n",
            "[I 2023-10-04 20:12:20,255] Trial 2 finished with value: 0.5780400037765503 and parameters: {'learning_rate': 0.004217201428257707}. Best is trial 1 with value: 0.6027200222015381.\n",
            "[I 2023-10-04 20:13:37,051] Trial 3 finished with value: 0.6431599855422974 and parameters: {'learning_rate': 0.009271834040398752}. Best is trial 3 with value: 0.6431599855422974.\n",
            "[I 2023-10-04 20:14:53,859] Trial 4 finished with value: 0.6279199719429016 and parameters: {'learning_rate': 0.0069070515383816864}. Best is trial 3 with value: 0.6431599855422974.\n",
            "[I 2023-10-04 20:16:09,991] Trial 5 finished with value: 0.59579998254776 and parameters: {'learning_rate': 0.003888551136100354}. Best is trial 3 with value: 0.6431599855422974.\n",
            "[I 2023-10-04 20:17:27,142] Trial 6 finished with value: 0.5735599994659424 and parameters: {'learning_rate': 0.0034405944715565234}. Best is trial 3 with value: 0.6431599855422974.\n",
            "[I 2023-10-04 20:18:42,447] Trial 7 finished with value: 0.5228000283241272 and parameters: {'learning_rate': 0.0011285704721295691}. Best is trial 3 with value: 0.6431599855422974.\n",
            "[I 2023-10-04 20:19:58,395] Trial 8 finished with value: 0.5320799946784973 and parameters: {'learning_rate': 0.001139562575797087}. Best is trial 3 with value: 0.6431599855422974.\n",
            "[I 2023-10-04 20:21:15,934] Trial 9 finished with value: 0.604640007019043 and parameters: {'learning_rate': 0.006771253874581079}. Best is trial 3 with value: 0.6431599855422974.\n",
            "[I 2023-10-04 20:22:31,278] Trial 10 finished with value: 0.5953599810600281 and parameters: {'learning_rate': 0.00988086494551896}. Best is trial 3 with value: 0.6431599855422974.\n",
            "[I 2023-10-04 20:23:47,768] Trial 11 finished with value: 0.6018000245094299 and parameters: {'learning_rate': 0.009365936067093867}. Best is trial 3 with value: 0.6431599855422974.\n",
            "[I 2023-10-04 20:25:03,736] Trial 12 finished with value: 0.6484000086784363 and parameters: {'learning_rate': 0.008327253998051015}. Best is trial 12 with value: 0.6484000086784363.\n",
            "[I 2023-10-04 20:26:18,299] Trial 13 finished with value: 0.6065999865531921 and parameters: {'learning_rate': 0.008476275053286134}. Best is trial 12 with value: 0.6484000086784363.\n",
            "[I 2023-10-04 20:27:33,160] Trial 14 finished with value: 0.6123600006103516 and parameters: {'learning_rate': 0.008161440313610263}. Best is trial 12 with value: 0.6484000086784363.\n",
            "[I 2023-10-04 20:28:50,263] Trial 15 finished with value: 0.5949199795722961 and parameters: {'learning_rate': 0.008483226763681259}. Best is trial 12 with value: 0.6484000086784363.\n",
            "[I 2023-10-04 20:30:06,830] Trial 16 finished with value: 0.6668800115585327 and parameters: {'learning_rate': 0.009741775202271735}. Best is trial 16 with value: 0.6668800115585327.\n",
            "[I 2023-10-04 20:31:22,121] Trial 17 finished with value: 0.6018800139427185 and parameters: {'learning_rate': 0.00994922114377978}. Best is trial 16 with value: 0.6668800115585327.\n",
            "[I 2023-10-04 20:32:37,828] Trial 18 finished with value: 0.6084399819374084 and parameters: {'learning_rate': 0.007542494010123213}. Best is trial 16 with value: 0.6668800115585327.\n",
            "[I 2023-10-04 20:33:53,787] Trial 19 finished with value: 0.59579998254776 and parameters: {'learning_rate': 0.005488100317246159}. Best is trial 16 with value: 0.6668800115585327.\n",
            "[I 2023-10-04 20:35:09,314] Trial 20 finished with value: 0.6088799834251404 and parameters: {'learning_rate': 0.008639577386264783}. Best is trial 16 with value: 0.6668800115585327.\n",
            "[I 2023-10-04 20:36:24,897] Trial 21 finished with value: 0.6421200037002563 and parameters: {'learning_rate': 0.009076903820004523}. Best is trial 16 with value: 0.6668800115585327.\n",
            "[I 2023-10-04 20:37:40,956] Trial 22 finished with value: 0.6608399748802185 and parameters: {'learning_rate': 0.009363307956741338}. Best is trial 16 with value: 0.6668800115585327.\n",
            "[I 2023-10-04 20:38:56,919] Trial 23 finished with value: 0.6521999835968018 and parameters: {'learning_rate': 0.009988124190603496}. Best is trial 16 with value: 0.6668800115585327.\n",
            "[I 2023-10-04 20:40:13,004] Trial 24 finished with value: 0.6490799784660339 and parameters: {'learning_rate': 0.009920402386393308}. Best is trial 16 with value: 0.6668800115585327.\n",
            "[I 2023-10-04 20:41:28,775] Trial 25 finished with value: 0.6607999801635742 and parameters: {'learning_rate': 0.00772993666515206}. Best is trial 16 with value: 0.6668800115585327.\n",
            "[I 2023-10-04 20:42:44,212] Trial 26 finished with value: 0.6339600086212158 and parameters: {'learning_rate': 0.007595246807570628}. Best is trial 16 with value: 0.6668800115585327.\n",
            "[I 2023-10-04 20:43:59,074] Trial 27 finished with value: 0.6471999883651733 and parameters: {'learning_rate': 0.00907916170348285}. Best is trial 16 with value: 0.6668800115585327.\n",
            "[I 2023-10-04 20:45:14,497] Trial 28 finished with value: 0.6122400164604187 and parameters: {'learning_rate': 0.007697351857884135}. Best is trial 16 with value: 0.6668800115585327.\n",
            "[I 2023-10-04 20:46:31,051] Trial 29 finished with value: 0.6346799731254578 and parameters: {'learning_rate': 0.009056005712280412}. Best is trial 16 with value: 0.6668800115585327.\n",
            "[I 2023-10-04 20:47:47,967] Trial 30 finished with value: 0.6054800152778625 and parameters: {'learning_rate': 0.007675621534153207}. Best is trial 16 with value: 0.6668800115585327.\n",
            "[I 2023-10-04 20:49:03,154] Trial 31 finished with value: 0.655239999294281 and parameters: {'learning_rate': 0.009991378127694671}. Best is trial 16 with value: 0.6668800115585327.\n",
            "[I 2023-10-04 20:50:19,659] Trial 32 finished with value: 0.6086000204086304 and parameters: {'learning_rate': 0.009155143593452346}. Best is trial 16 with value: 0.6668800115585327.\n",
            "[I 2023-10-04 20:51:35,010] Trial 33 finished with value: 0.6353999972343445 and parameters: {'learning_rate': 0.00947809243574283}. Best is trial 16 with value: 0.6668800115585327.\n",
            "[I 2023-10-04 20:52:50,875] Trial 34 finished with value: 0.658240020275116 and parameters: {'learning_rate': 0.008590554858940112}. Best is trial 16 with value: 0.6668800115585327.\n",
            "[I 2023-10-04 20:54:06,878] Trial 35 finished with value: 0.5732399821281433 and parameters: {'learning_rate': 0.006222645553073168}. Best is trial 16 with value: 0.6668800115585327.\n",
            "[I 2023-10-04 20:55:22,597] Trial 36 finished with value: 0.6033599972724915 and parameters: {'learning_rate': 0.00811226947252846}. Best is trial 16 with value: 0.6668800115585327.\n",
            "[I 2023-10-04 20:56:38,896] Trial 37 finished with value: 0.6118000149726868 and parameters: {'learning_rate': 0.00886620058933764}. Best is trial 16 with value: 0.6668800115585327.\n",
            "[I 2023-10-04 20:57:54,020] Trial 38 finished with value: 0.5328800082206726 and parameters: {'learning_rate': 0.006867842851110317}. Best is trial 16 with value: 0.6668800115585327.\n",
            "[I 2023-10-04 20:59:09,674] Trial 39 finished with value: 0.6564800143241882 and parameters: {'learning_rate': 0.007187388508956161}. Best is trial 16 with value: 0.6668800115585327.\n",
            "[I 2023-10-04 21:00:26,622] Trial 40 finished with value: 0.5933600068092346 and parameters: {'learning_rate': 0.008035355447219884}. Best is trial 16 with value: 0.6668800115585327.\n",
            "[I 2023-10-04 21:01:42,284] Trial 41 finished with value: 0.5932000279426575 and parameters: {'learning_rate': 0.007343689328153139}. Best is trial 16 with value: 0.6668800115585327.\n",
            "[I 2023-10-04 21:02:57,916] Trial 42 finished with value: 0.593559980392456 and parameters: {'learning_rate': 0.006254679305173293}. Best is trial 16 with value: 0.6668800115585327.\n",
            "[I 2023-10-04 21:04:14,945] Trial 43 finished with value: 0.6250399947166443 and parameters: {'learning_rate': 0.00851227315046853}. Best is trial 16 with value: 0.6668800115585327.\n",
            "[I 2023-10-04 21:05:32,336] Trial 44 finished with value: 0.635640025138855 and parameters: {'learning_rate': 0.009457238582117519}. Best is trial 16 with value: 0.6668800115585327.\n",
            "[I 2023-10-04 21:06:48,854] Trial 45 finished with value: 0.6411200165748596 and parameters: {'learning_rate': 0.00861831818635651}. Best is trial 16 with value: 0.6668800115585327.\n",
            "[I 2023-10-04 21:08:04,956] Trial 46 finished with value: 0.6169599890708923 and parameters: {'learning_rate': 0.0070825749131093185}. Best is trial 16 with value: 0.6668800115585327.\n",
            "[I 2023-10-04 21:09:23,045] Trial 47 finished with value: 0.6438400149345398 and parameters: {'learning_rate': 0.009454683788182153}. Best is trial 16 with value: 0.6668800115585327.\n",
            "[I 2023-10-04 21:10:38,489] Trial 48 finished with value: 0.6064800024032593 and parameters: {'learning_rate': 0.008099744480490171}. Best is trial 16 with value: 0.6668800115585327.\n",
            "[I 2023-10-04 21:11:55,481] Trial 49 finished with value: 0.6045600175857544 and parameters: {'learning_rate': 0.008799570367405148}. Best is trial 16 with value: 0.6668800115585327.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression - Best Parameters: {'C': 97285.13274100378}\n",
            "Logistic Regression - Best Accuracy: 0.5116\n",
            "CNN(Adam optimizer) - Best Parameters: {'learning_rate': 0.0031619903466632814}\n",
            "CNN(Adam optimizer) - Best Accuracy: 0.8610\n",
            "CNN(SGD optimizer) - Best Parameters: {'learning_rate': 0.009741775202271735}\n",
            "CNN(SGD optimizer) - Best Accuracy: 0.6669\n"
          ]
        }
      ],
      "source": [
        "# 3. SGD and Adam optimizers\n",
        "import optuna\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from keras.optimizers import SGD, Adam\n",
        "\n",
        "# Define the objective function for Logistic Regression\n",
        "def objective_lr(trial):\n",
        "    C = trial.suggest_float('C', 1e-5, 1e5)\n",
        "\n",
        "    # Initialize the Logistic Regression model\n",
        "    lr_model = LogisticRegression(max_iter=1000, C=C)\n",
        "\n",
        "    # Train the model\n",
        "    lr_model.fit(X_train, train_labels)\n",
        "\n",
        "    # Evaluate the model\n",
        "    accuracy = lr_model.score(X_test, test_labels)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "# Define the objective function for CNN\n",
        "def objective_cnn_adam(trial):\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2)\n",
        "\n",
        "    # Initialize the CNN model\n",
        "    cnn_model = Sequential([\n",
        "        Embedding(input_dim=max_words, output_dim=50, input_length=maxlen),\n",
        "        Conv1D(128, 5, activation='relu'),\n",
        "        GlobalMaxPooling1D(),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Compile the model with Adam optimizer\n",
        "    optimizer = Adam(learning_rate=learning_rate)  # Use Adam optimizer here\n",
        "    cnn_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Convert sparse matrices to dense\n",
        "    X_train_dense = X_train\n",
        "    X_test_dense = X_test\n",
        "\n",
        "    # Train the model\n",
        "    cnn_model.fit(X_train_dense, train_labels, epochs=10, batch_size=32, validation_data=(X_test_dense, test_labels), verbose=0)\n",
        "\n",
        "    # Evaluate the model\n",
        "    _, accuracy = cnn_model.evaluate(X_test_dense, test_labels, verbose=0)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "# Define the objective function for CNN with SGD optimizer\n",
        "def objective_cnn_sgd(trial):\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2)\n",
        "\n",
        "    # Initialize the CNN model\n",
        "    cnn_model = Sequential([\n",
        "        Embedding(input_dim=max_words, output_dim=50, input_length=maxlen),\n",
        "        Conv1D(128, 5, activation='relu'),\n",
        "        GlobalMaxPooling1D(),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Compile the model with SGD optimizer\n",
        "    optimizer = SGD(learning_rate=learning_rate)  # Use SGD optimizer here\n",
        "    cnn_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Convert sparse matrices to dense\n",
        "    X_train_dense = X_train\n",
        "    X_test_dense = X_test\n",
        "\n",
        "    # Train the model\n",
        "    cnn_model.fit(X_train_dense, train_labels, epochs=10, batch_size=32, validation_data=(X_test_dense, test_labels), verbose=0)\n",
        "\n",
        "    # Evaluate the model\n",
        "    _, accuracy = cnn_model.evaluate(X_test_dense, test_labels, verbose=0)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "# Create a study for Logistic Regression\n",
        "study_lr = optuna.create_study(direction='maximize')\n",
        "study_lr.optimize(objective_lr, n_trials=5)\n",
        "\n",
        "# Get the best hyperparameters for Logistic Regression\n",
        "best_params_lr = study_lr.best_params\n",
        "best_accuracy_lr = study_lr.best_value\n",
        "\n",
        "# Create a study for CNN using adam optimizer\n",
        "study_cnn_adam = optuna.create_study(direction='maximize')\n",
        "study_cnn_adam.optimize(objective_cnn_adam, n_trials=50)\n",
        "\n",
        "# Create a study for CNN using adam optimizer\n",
        "study_cnn_sgd = optuna.create_study(direction='maximize')\n",
        "study_cnn_sgd.optimize(objective_cnn_sgd, n_trials=50)\n",
        "\n",
        "# Get the best hyperparameters for CNN-adam\n",
        "best_params_cnn_adam = study_cnn_adam.best_params\n",
        "best_accuracy_cnn_adam = study_cnn_adam.best_value\n",
        "\n",
        "# Get the best hyperparameters for CNN-sgd\n",
        "best_params_cnn_sgd = study_cnn_sgd.best_params\n",
        "best_accuracy_cnn_sgd = study_cnn_sgd.best_value\n",
        "\n",
        "# Print the results\n",
        "print(f\"Logistic Regression - Best Parameters: {best_params_lr}\")\n",
        "print(f\"Logistic Regression - Best Accuracy: {best_accuracy_lr:.4f}\")\n",
        "\n",
        "print(f\"CNN(Adam optimizer) - Best Parameters: {best_params_cnn_adam}\")\n",
        "print(f\"CNN(Adam optimizer) - Best Accuracy: {best_accuracy_cnn_adam:.4f}\")\n",
        "\n",
        "print(f\"CNN(SGD optimizer) - Best Parameters: {best_params_cnn_sgd}\")\n",
        "print(f\"CNN(SGD optimizer) - Best Accuracy: {best_accuracy_cnn_sgd:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comparison of Results:\n",
        "\n",
        "## Logistic Regression\n",
        "\n",
        "- Best Parameters: {'C': 97285.13274100378}\n",
        "- Accuracy: 0.5116\n",
        "\n",
        "This result is slightly better than random guessing. The 'C' parameter denotes the inverse of regularization strength, with smaller values specifying stronger regularization.\n",
        "\n",
        "## CNN (Adam optimizer)\n",
        "\n",
        "- Best Parameters: {'learning_rate': 0.0031619903466632814}\n",
        "- Accuracy: 0.8610\n",
        "\n",
        "This suggests that Adam provides a much better accuracy for the given problem using the Convolutional Neural Network (CNN) framework.\n",
        "\n",
        "## CNN (SGD optimizer)\n",
        "\n",
        "- Best Parameters: {'learning_rate': 0.009741775202271735}\n",
        "- Accuracy: 0.6669\n",
        "\n",
        "Although SGD is a widely used optimizer, in this instance, it lagged behind Adam in terms of accuracy.\n",
        "\n",
        "# Replication and Merits of Adam vs. SGD:\n",
        "\n",
        "The paper suggests that while Adam tends to converge faster than vanilla SGD and SGD with momentum, it may not generalize as well. However, well-tuned SGD with momentum can surpass Adam in terms of both training and test error. This might align with the results provided, where Adam has higher accuracy than SGD in the CNN models.\n",
        "\n",
        "The claim of the “marginal value” of adaptive gradient methods (as cited in [222]) did not deter the growing use of Adam, especially in other domains such as GANs and reinforcement learning.\n",
        "\n",
        "# Hyperparameter Optimization:\n",
        "\n",
        "The provided results indicate that hyperparameter tuning was performed, given the 'Best Parameters' for each model. The paper remarks on the \"tunability\" of Adam, suggesting that it is more flexible and less sensitive to hyperparameter choices compared to SGD.\n",
        "\n",
        "As mentioned on page 24, hyperparameter optimization might render the empirical results less relevant. This is because optimized hyperparameters can overshadow the innate benefits of an algorithm, making it challenging to make general comparisons.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
